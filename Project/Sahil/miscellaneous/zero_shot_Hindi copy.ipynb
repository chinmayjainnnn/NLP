{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"    # Free GPU\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37ed6b12a784f239d105499abfaaf05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    cache_dir=\"/data5/home/sahilm/NLP_Project/Llama_2_13b_chat_hf\"\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", \n",
    "                                          cache_dir=\"/data5/home/sahilm/NLP_Project/Llama_2_13b_chat_hf\"\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moj\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 89.38 MiB is free. Including non-PyTorch memory, this process has 39.30 GiB memory in use. Of the allocated memory 38.29 GiB is allocated by PyTorch, and 12.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoj\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 89.38 MiB is free. Including non-PyTorch memory, this process has 39.30 GiB memory in use. Of the allocated memory 38.29 GiB is allocated by PyTorch, and 12.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model = DataParallel(model)\n",
    "    print(\"Moj\")\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_parallel = model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def calculate_bleu_score(reference, candidate):\n",
    "    # Tokenize reference and candidate strings\n",
    "    reference_tokens = word_tokenize(reference.lower())\n",
    "    candidate_tokens = word_tokenize(candidate.lower())\n",
    "    # print(reference_tokens)\n",
    "    # print([reference_tokens])\n",
    "    # print(candidate_tokens)\n",
    "    # Calculate BLEU score\n",
    "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens,smoothing_function=SmoothingFunction().method1)\n",
    "    \n",
    "    return bleu_score\n",
    "\n",
    "# Example usage:\n",
    "reference = \"मैं क्रिकेट खेलना चाहता हूं\"\n",
    "candidate = \"मैं क्रिकेट खेलना चाहता हूं\"\n",
    "score = calculate_bleu_score(reference, candidate)\n",
    "print(\"BLEU Score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "english_file = '/data5/home/sahilm/NLP_Project/Dataset/Dest/final_data/en-hi/train.en'\n",
    "gujarati_file = '/data5/home/sahilm/NLP_Project/Dataset/Dest/final_data/en-hi/train.hi'\n",
    "\n",
    "english_sentences = []\n",
    "gujarati_sentences = []\n",
    "total_sentences = 5000\n",
    "\n",
    "with open(english_file, 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= total_sentences:\n",
    "            break\n",
    "        english_sentences.append(line)\n",
    "\n",
    "with open(gujarati_file, 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= total_sentences:\n",
    "            break\n",
    "        gujarati_sentences.append(line)\n",
    "\n",
    "\n",
    "print(len(english_sentences))\n",
    "print(len(gujarati_sentences))\n",
    "\n",
    "# total_sentences = 5000\n",
    "# english_sentences = english_sentences[:total_sentences]\n",
    "# gujarati_sentences = gujarati_sentences[:total_sentences]\n",
    "\n",
    "english_sentences = [sentence.rstrip('\\n') for sentence in english_sentences]\n",
    "gujarati_sentences = [sentence.rstrip('\\n') for sentence in gujarati_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     print(english_sentences[i])\n",
    "#     print(gujarati_sentences[i])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264.0\n",
      "253.0\n"
     ]
    }
   ],
   "source": [
    "PERCENTILE = 97\n",
    "print(np.percentile([len(x) for x in english_sentences], PERCENTILE))\n",
    "print(np.percentile([len(x) for x in gujarati_sentences], PERCENTILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     print(english_sentences[i])\n",
    "#     print(gujarati_sentences[i])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextDataSet(Dataset):\n",
    "    def __init__(self, english_sentences, gujarati_sentences):\n",
    "        super().__init__\n",
    "        self.english_sentences = english_sentences\n",
    "        self.gujarati_sentences = gujarati_sentences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.english_sentences[idx], self.gujarati_sentences[idx]\n",
    "dataset = TextDataSet(english_sentences,gujarati_sentences)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=150\n",
    "train_loader = DataLoader(dataset,batch_size)\n",
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     # Do something with the batch\n",
    "#     # print(batch)\n",
    "#     # print(len(batch))\n",
    "#     # print(len(batch[0]))\n",
    "#     # print(len(batch[1]))\n",
    "#     for i in range(len(batch[0])):\n",
    "#         print(batch[0][i])\n",
    "#         print(batch[1][i])\n",
    "#         print()\n",
    "#         if i==2:\n",
    "#             break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model_without_parallel,\n",
    "    tokenizer = tokenizer,\n",
    "    torch_dtype = torch.float16,\n",
    "    device = 0 if device.type == \"cuda\" else -1,\n",
    "    batch_size=128,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH: In reply, Pakistan got off to a solid start.\n",
      "HINDI: जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\n",
      "OUTPUT: पाकिस्तान का सुरकSHA सタर्प कर दिया.\n",
      "BLEU Score: 0.02151810250197126\n",
      "Average BLEU Score: 0.02151810250197126\n",
      "\n",
      "ENGLISH: The European Union has seven principal decision-making bodies, its institutions: the European Parliament, the European Council, the Council of the European Union, the European Commission, the Court of Justice of the European Union, the European Central Bank and the European Court of Auditors.\n",
      "HINDI: यूरोपीय संघ के महत्वपूर्ण संस्थानों में यूरोपियन कमीशन, यूरोपीय संसद, यूरोपीय संघ परिषद, यूरोपीय न्यायलय एवं यूरोपियन सेंट्रल बैंक इत्यादि शामिल हैं।\n",
      "OUTPUT: एशिया यूनियन ने सप्ताह का समानुभूत पालित करने की सारी नियमाना संस्थान: ईश्वर पार्लिमेंट, ईश्वर कौंसल, संस्थान ऑफ ईश्वर उन्नायन, एसको जस्टिस ऑफ ईश्वर कान्सल, एसको जस्टिस ऑफ ईश्वर एक्सामा, एसको जस\n",
      "BLEU Score: 0.006414921514603761\n",
      "Average BLEU Score: 0.01396651200828751\n",
      "\n",
      "ENGLISH: The Congress leader represents Sivaganga Lok Sabha segment from Tamil Nadu.\n",
      "HINDI: कांग्रेस नेता तमिलनाडु से शिवगंगा लोकसभा क्षेत्र का प्रतिनिधित्व करते हैं.\n",
      "OUTPUT: कांग्रेस नेता सिवागंगा लोक सभा संस्थान के संबंध में तामिलनाडु से रखता है.\n",
      "BLEU Score: 0.03592143420342892\n",
      "Average BLEU Score: 0.021284819406667977\n",
      "\n",
      "Max BLEU Score:  0.03592143420342892\n"
     ]
    }
   ],
   "source": [
    "total_bleu_score = 0\n",
    "max_bleu_score = 0\n",
    "with open(\"zero_shot_Hindi_output.txt\", 'w') as f:\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Do something with the batch\n",
    "        # print(batch)\n",
    "        # print(len(batch))\n",
    "        # print(len(batch[0]))\n",
    "        # print(len(batch[1]))\n",
    "        for i in range(len(batch[0])):\n",
    "            text = batch[0][i].strip()\n",
    "            # text = \"I want to play cricket\"\n",
    "            \n",
    "            reference = batch[1][i].strip()\n",
    "            # reference = \"मैं क्रिकेट खेलना चाहता हूं\"\n",
    "            \n",
    "            print(\"ENGLISH: \" + text)\n",
    "            print(\"ENGLISH: \" + text, file=f)\n",
    "            print(\"HINDI: \" + reference)\n",
    "            print(\"HINDI: \" + reference, file=f)\n",
    "            \n",
    "            template = f\"\"\"Translate the following text to Hindi:\\nText: {text}\\nOutput: \"\"\"    \n",
    "            \n",
    "            sequences = pipeline(\n",
    "                template,\n",
    "                do_sample = True,\n",
    "                top_k = 10,\n",
    "                num_return_sequences = 1,\n",
    "                eos_token_id = tokenizer.eos_token_id,\n",
    "                max_length = max_seq_length,\n",
    "                truncation = True\n",
    "            )\n",
    "            output = sequences[0]['generated_text']\n",
    "\n",
    "            sentences_after_output = re.findall(r'Output:(.*)', output)\n",
    "            output = sentences_after_output[0].strip()\n",
    "            print(\"OUTPUT: \" + output)\n",
    "            print(\"OUTPUT: \" + output,file=f)\n",
    "            bleu_score = calculate_bleu_score(reference, output)\n",
    "            max_bleu_score = max(bleu_score,max_bleu_score)\n",
    "            total_bleu_score += bleu_score\n",
    "            print(\"BLEU Score:\", bleu_score)\n",
    "            print(\"BLEU Score:\", bleu_score,file=f)\n",
    "            print(\"Average BLEU Score:\", total_bleu_score/(i+1))\n",
    "            print(\"Average BLEU Score:\", total_bleu_score/(i+1),file=f)\n",
    "            print()\n",
    "            print(\"\",file=f)\n",
    "            if i==2:\n",
    "                break\n",
    "        break\n",
    "\n",
    "    print(\"Max BLEU Score: \", max_bleu_score)\n",
    "    print(\"Max BLEU Score: \", max_bleu_score,file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
